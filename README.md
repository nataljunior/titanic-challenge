# titanic-challenge
Exploring different techniques to compare results using the famous Kaggle Competition
# Titanic Disaster - Machine Learning Project: https://www.kaggle.com/competitions/titanic/overview
# Datasets: https://www.kaggle.com/competitions/titanic/data

This project aims to explore various techniques for predicting survival rates in the Titanic disaster using machine learning models. The dataset used is from the famous Kaggle Competition - Titanic: Machine Learning from Disaster.

## Techniques Used
- **Feature Engineering:** Employed various methods to create new features from the dataset to improve model performance.
- **Decision Trees:** A machine learning algorithm used for classification tasks by creating a flowchart-like tree structure.
- **Random Forest:** An ensemble learning method utilizing multiple decision trees to improve predictive accuracy and control overfitting.
- **XGBoost:** An optimized gradient boosting algorithm known for its speed and performance in regression, classification, and ranking problems.
- **CatBoost:** A gradient boosting library that handles categorical variables automatically and efficiently.

## Results
- Decision Trees: 0.67703
- Random Forest: 0.75358
- XGBoost: 0.76076
- CatBoost: 0.75119

## Next Steps
Future steps involve experimenting with different approaches to feature engineering. Suggestions for further improvements include:
- Advanced feature selection methods.
- Hyperparameter tuning for each model.
- Ensemble methods to combine the strengths of different models.
- Trying different algorithms or stacking methods for improved performance.

Feel free to contribute, experiment with additional techniques, or suggest improvements to enhance the predictive accuracy for this challenge.

Your contributions and suggestions are welcomed and appreciated!

